#!/usr/bin/env python3
"""
Test script to verify RAFT memory optimization.
Simulates the OOM scenario with 864x1536 resolution.
"""

import torch
import torch.nn.functional as F
import numpy as np
import sys
import os

# Add current directory to path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

def test_memory_efficiency():
    print("ðŸ§ª Testing RAFT Memory Optimization")
    print("=" * 60)
    
    # Simulate the problematic resolution
    h_orig, w_orig = 864, 1536
    print(f"Original resolution: {h_orig}x{w_orig} ({h_orig * w_orig:,} pixels)")
    
    # Calculate scale factor based on our logic
    total_pixels = h_orig * w_orig
    if total_pixels > 1024 * 1024:  # > 1MP
        scale_factor = 0.5
        print(f"âœ… Scale factor selected: {scale_factor} (resolution > 1MP)")
    else:
        scale_factor = 1.0
        print(f"âœ… Scale factor selected: {scale_factor}")
    
    # Calculate downscaled dimensions
    h_small = int(h_orig * scale_factor)
    w_small = int(w_orig * scale_factor)
    print(f"Downscaled resolution: {h_small}x{w_small} ({h_small * w_small:,} pixels)")
    
    # Calculate memory savings
    if scale_factor < 1.0:
        orig_pixels = h_orig * w_orig
        small_pixels = h_small * w_small
        memory_reduction = 1.0 - (small_pixels / orig_pixels)
        print(f"ðŸ“‰ Memory reduction: {memory_reduction:.1%}")
        print(f"ðŸ“Š Pixel count reduction: {orig_pixels:,} â†’ {small_pixels:,}")
    
    # Simulate tensor sizes
    print("\nðŸ“¦ Simulating tensor memory usage:")
    
    # Original video tensor (simulating 3 frames, 3 channels)
    b, t, c = 1, 3, 3
    original_tensor_size = b * t * c * h_orig * w_orig
    original_memory_mb = (original_tensor_size * 4) / (1024 ** 2)  # FP32
    
    print(f"Original tensor (FP32): {original_memory_mb:.2f} MB")
    
    if scale_factor < 1.0:
        small_tensor_size = b * t * c * h_small * w_small
        small_memory_mb = (small_tensor_size * 4) / (1024 ** 2)  # FP32
        print(f"Downscaled tensor (FP32): {small_memory_mb:.2f} MB")
        print(f"Memory saved: {original_memory_mb - small_memory_mb:.2f} MB")
    
    # Test the actual interpolation logic
    print("\nðŸ”§ Testing interpolation logic...")
    
    # Create dummy tensor
    dummy_tensor = torch.randn(b * t, c, h_orig, w_orig)
    print(f"Dummy tensor shape: {dummy_tensor.shape}")
    
    if scale_factor < 1.0:
        # Downscale
        downscaled = F.interpolate(dummy_tensor.float(),
                                  size=(h_small, w_small),
                                  mode='bilinear',
                                  align_corners=False)
        print(f"Downscaled shape: {downscaled.shape}")
        
        # Create dummy flow (simulating RAFT output)
        dummy_flow = torch.randn(b * (t-1), 2, h_small, w_small)
        print(f"Dummy flow shape: {dummy_flow.shape}")
        
        # Upscale flow
        upscaled = F.interpolate(dummy_flow,
                                size=(h_orig, w_orig),
                                mode='bilinear',
                                align_corners=False)
        
        # Scale flow values
        upscaled = upscaled * (1.0 / scale_factor)
        print(f"Upscaled flow shape: {upscaled.shape}")
        print(f"Flow scaling factor applied: {1.0 / scale_factor}")
        
        # Verify scaling
        flow_mean_original = dummy_flow.mean().item()
        flow_mean_scaled = upscaled.mean().item()
        print(f"Flow mean before scaling: {flow_mean_original:.4f}")
        print(f"Flow mean after scaling: {flow_mean_scaled:.4f}")
        print(f"Expected scaling ratio: {1.0 / scale_factor}")
        print(f"Actual scaling ratio: {flow_mean_scaled / flow_mean_original:.4f}")
    
    print("\nâœ… Test completed successfully!")
    print("=" * 60)
    
    # Summary
    print("\nðŸ“‹ OPTIMIZATION SUMMARY:")
    print(f"1. Resolution: {h_orig}x{w_orig} â†’ {h_small}x{w_small}")
    print(f"2. Scale factor: {scale_factor}")
    if scale_factor < 1.0:
        print(f"3. Memory reduction: ~{memory_reduction:.1%}")
        print(f"4. Expected VRAM usage for RAFT: {small_memory_mb:.1f} MB (was {original_memory_mb:.1f} MB)")
        print(f"5. Should fit in 12.6 GB VRAM: {'âœ… YES' if small_memory_mb < 12000 else 'âŒ NO'}")
    else:
        print("3. No downscaling needed (resolution already manageable)")
    
    return True

def test_imports():
    """Test that all required imports work."""
    print("\nðŸ” Testing imports...")
    
    try:
        from model.modules.flow_comp_raft import RAFT_bi
        print("âœ… RAFT_bi import successful")
    except ImportError as e:
        print(f"âŒ RAFT_bi import failed: {e}")
        return False
    
    # Skip problematic imports for this test
    # They're not needed for RAFT optimization verification
    print("âš ï¸ Skipping torchvision-dependent imports for this test")
    print("âš ï¸ (Not needed for RAFT optimization verification)")
    
    return True

if __name__ == "__main__":
    print("ðŸš€ Starting RAFT Optimization Test")
    print("=" * 60)
    
    # Check CUDA availability
    cuda_available = torch.cuda.is_available()
    print(f"CUDA Available: {'âœ… YES' if cuda_available else 'âŒ NO'}")
    
    if cuda_available:
        gpu_name = torch.cuda.get_device_name(0)
        total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        print(f"GPU: {gpu_name}")
        print(f"VRAM: {total_memory:.1f} GB")
    
    # Test imports
    if not test_imports():
        print("âŒ Some imports failed. Check your environment.")
        sys.exit(1)
    
    # Test memory efficiency
    test_memory_efficiency()
    
    print("\nðŸŽ¯ All tests passed! The optimization should prevent OOM errors.")
    print("\nNext steps:")
    print("1. Run actual inference with: python inference_core.py")
    print("2. Monitor VRAM usage during RAFT phase")
    print("3. Verify no OOM occurs at 864x1536 resolution")
